# -*- coding: utf-8 -*-
"""Netowrk_data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xqL5LzCJZ3uK2-hLrBDoMx5VCcY-0zJv
"""

!pip install researchpy

import statistics as st # Built in Python library for descriptive statistics
import pandas as pd  # Data manipulation and analysis library
import researchpy as rp  # Open source library focused on univariate and bivariate analysis
import numpy as np  # General purpose array processing package
import seaborn as sns  # Data visualization library based on matplotlib
import matplotlib.pyplot as plt  # Data visualization library
import scipy.stats as spy  # Statistical library from Scipy
import statistics as st # Built in Python library for descriptive statistics
import pandas as pd  # Data manipulation and analysis library
import researchpy as rp  # Open source library focused on univariate and bivariate analysis
import numpy as np  # General purpose array processing package
import seaborn as sns  # Data visualization library based on matplotlib
import matplotlib.pyplot as plt  # Data visualization library
import scipy.stats as spy  # Statistical library from Scipy
from scipy.stats import norm, kurtosis, laplace, semicircular
from scipy import stats
from scipy.stats import gmean, hmean, trim_mean

# Loading dataset1
# https://archive.ics.uci.edu/ml/datasets/Mammographic+Mass
# Missing Values? Yes
df = pd.read_csv('FR_edges.csv')
df.shape

import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt

# Load your Twitch Social Network dataset into a DataFrame
# Replace 'twitch_social_network.csv' with your actual file path
df = pd.read_csv('FR_edges.csv')
# Take only the first 30 rows of the dataset
df_subset = df.head(30)

# Create a directed graph from the dataset
G = nx.from_pandas_edgelist(df, 'from', 'to', create_using=nx.DiGraph())

# Plot the network
plt.figure(figsize=(12, 8))
pos = nx.spring_layout(G)
nx.draw(G, pos, with_labels=True, font_size=8, node_size=20, node_color='skyblue', font_color='black', edge_color='gray', alpha=0.7)
plt.title('Twitch Social Network Visualization')
plt.show()

import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt

# Load your Twitch Social Network dataset into a DataFrame
# Replace 'twitch_social_network.csv' with your actual file path
df = pd.read_csv('FR_edges.csv')

# Create a directed graph from the dataset
G = nx.from_pandas_edgelist(df, 'from', 'to', create_using=nx.DiGraph())

# Find the 50 nodes with the highest degree (most connections)
top_50_nodes = sorted(G.degree, key=lambda x: x[1], reverse=True)[:50]

# Create a subgraph with only the top 50 nodes and their edges
subgraph = G.subgraph([node for node, _ in top_50_nodes])

# Plot the network with multiple layouts for the top 50 nodes
plt.figure(figsize=(18, 6))

# Spring layout
plt.subplot(1, 3, 1)
pos_spring = nx.spring_layout(subgraph)
nx.draw(subgraph, pos_spring, with_labels=True, font_size=8, node_size=20, node_color='skyblue', font_color='black', edge_color='gray', alpha=0.7)
plt.title('Spring Layout (Top 50 Nodes)')

# Shell layout
plt.subplot(1, 3, 2)
pos_shell = nx.shell_layout(subgraph)
nx.draw(subgraph, pos_shell, with_labels=True, font_size=8, node_size=20, node_color='skyblue', font_color='black', edge_color='gray', alpha=0.7)
plt.title('Shell Layout (Top 50 Nodes)')

# Circular layout
plt.subplot(1, 3, 3)
pos_circular = nx.circular_layout(subgraph)
nx.draw(subgraph, pos_circular, with_labels=True, font_size=8, node_size=20, node_color='skyblue', font_color='black', edge_color='gray', alpha=0.7)
plt.title('Circular Layout (Top 50 Nodes)')

plt.tight_layout()
plt.show()

import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt

# Load your Twitch Social Network dataset into a DataFrame
# Replace 'twitch_social_network.csv' with your actual file path
df = pd.read_csv('FR_edges.csv')

# Create a directed graph from the dataset
G = nx.from_pandas_edgelist(df, 'from', 'to', create_using=nx.DiGraph())

# Find the 50 nodes with the highest degree (most connections)
top_50_nodes = sorted(G.degree, key=lambda x: x[1], reverse=True)[:50]

# Create a subgraph with only the top 50 nodes and their edges
subgraph = G.subgraph([node for node, _ in top_50_nodes])

# Plot the network with various layouts for the top 50 nodes
plt.figure(figsize=(18, 6))

# Kamada-Kawai layout
plt.subplot(1, 3, 1)
pos_kamada = nx.kamada_kawai_layout(subgraph)
nx.draw(subgraph, pos_kamada, with_labels=True, font_size=8, node_size=20, node_color='skyblue', font_color='black', edge_color='gray', alpha=0.7)
plt.title('Kamada-Kawai Layout (Top 50 Nodes)')

# Random layout
plt.subplot(1, 3, 2)
pos_random = nx.random_layout(subgraph)
nx.draw(subgraph, pos_random, with_labels=True, font_size=8, node_size=20, node_color='skyblue', font_color='black', edge_color='gray', alpha=0.7)
plt.title('Random Layout (Top 50 Nodes)')

# Spectral layout
plt.subplot(1, 3, 3)
pos_spectral = nx.spectral_layout(subgraph)
nx.draw(subgraph, pos_spectral, with_labels=True, font_size=8, node_size=20, node_color='skyblue', font_color='black', edge_color='gray', alpha=0.7)
plt.title('Spectral Layout (Top 50 Nodes)')

plt.tight_layout()
plt.show()

import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt

# Load your Twitch Social Network dataset into a DataFrame
# Replace 'twitch_social_network.csv' with your actual file path
df = pd.read_csv('FR_edges.csv')

# Create a directed graph from the dataset
G = nx.from_pandas_edgelist(df, 'from', 'to', create_using=nx.DiGraph())

# Find the 50 nodes with the highest degree (most connections)
top_50_nodes = sorted(G.degree, key=lambda x: x[1], reverse=True)[:50]

# Create a subgraph with only the top 50 nodes and their edges
subgraph = G.subgraph([node for node, _ in top_50_nodes])

# Plot the network with various layouts for the top 50 nodes
plt.figure(figsize=(18, 6))

# Kamada-Kawai layout
plt.subplot(1, 3, 1)
pos_kamada = nx.kamada_kawai_layout(subgraph)
nx.draw(subgraph, pos_kamada, with_labels=True, font_size=8, node_size=20, node_color='skyblue', font_color='black', edge_color='gray', alpha=0.7)
plt.title('Kamada-Kawai Layout (Top 50 Nodes)')

# Random layout
plt.subplot(1, 3, 2)
pos_random = nx.random_layout(subgraph)
nx.draw(subgraph, pos_random, with_labels=True, font_size=8, node_size=20, node_color='skyblue', font_color='black', edge_color='gray', alpha=0.7)
plt.title('Random Layout (Top 50 Nodes)')

# Spectral layout with a scale factor (adjust the scale as needed)
plt.subplot(1, 3, 3)
pos_spectral = nx.spectral_layout(subgraph, scale=10)
nx.draw(subgraph, pos_spectral, with_labels=True, font_size=8, node_size=20, node_color='skyblue', font_color='black', edge_color='gray', alpha=0.7)
plt.title('Spectral Layout (Top 50 Nodes)')

plt.tight_layout()
plt.show()

import pandas as pd
import networkx as nx
import numpy as np
import matplotlib.pyplot as plt

# Load your Twitch Social Network dataset into a DataFrame
# Replace 'twitch_social_network.csv' with your actual file path
df = pd.read_csv('FR_edges.csv')

# Create a directed graph from the dataset
G = nx.from_pandas_edgelist(df, 'from', 'to', create_using=nx.DiGraph())

# Create an adjacency matrix
adjacency_matrix = nx.to_numpy_array(G, dtype=np.int)

# Visualize the adjacency matrix as a heatmap
plt.figure(figsize=(10, 8))
plt.imshow(adjacency_matrix, cmap='viridis', interpolation='none', aspect='equal')
plt.colorbar()
plt.title('Adjacency Matrix Heatmap')
plt.xlabel('Nodes')
plt.ylabel('Nodes')
plt.show()

import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt

# Load your Twitch Social Network dataset into a DataFrame
# Replace 'twitch_social_network.csv' with your actual file path
df = pd.read_csv('FR_edges.csv')

# Create a directed graph from the dataset
G = nx.from_pandas_edgelist(df, 'from', 'to', create_using=nx.DiGraph())


# Create a binary adjacency matrix (0s and 1s)
adjacency_matrix = nx.adjacency_matrix(G).toarray()

# Visualize the 0s and 1s in the adjacency matrix
plt.matshow(adjacency_matrix, cmap='binary')
plt.title('Binary Adjacency Matrix')
plt.xlabel('Nodes')
plt.ylabel('Nodes')
plt.show()

# Install the required system packages
!apt-get install -y graphviz libgraphviz-dev pkg-config

# Install the pygraphviz Python package
!pip install pygraphviz

import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
from networkx.drawing.nx_agraph import graphviz_layout

# Load your Twitch Social Network dataset into a DataFrame
# Replace 'twitch_social_network.csv' with your actual file path
df = pd.read_csv('FR_edges.csv')

# Create a directed graph from the dataset
G = nx.from_pandas_edgelist(df, 'from', 'to', create_using=nx.DiGraph())

# Find the 50 nodes with the highest degree (most connections)
top_50_nodes = sorted(G.degree, key=lambda x: x[1], reverse=True)[:50]

# Create a subgraph with only the top 50 nodes and their edges
subgraph = G.subgraph([node for node, _ in top_50_nodes])

# Create a binary tree structure for visualization
tree = nx.bfs_tree(subgraph, source=top_50_nodes[0][0], depth_limit=2)

# Visualize the binary tree
pos = graphviz_layout(tree, prog="dot")
nx.draw(tree, pos, with_labels=True, font_size=8, node_size=300, node_color='skyblue', font_color='black', edge_color='gray', alpha=0.7)
plt.title('Binary Tree Visualization (Top 50 Degree Nodes)')
plt.show()

import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
from networkx.algorithms import community

# Load your Twitch Social Network dataset into a DataFrame
# Replace 'twitch_social_network.csv' with your actual file path
df = pd.read_csv('FR_edges.csv')

# Create a directed graph from the "from" and "to" columns
G = nx.from_pandas_edgelist(df, 'from', 'to', create_using=nx.DiGraph())

# 1. Degree Distribution
degrees = dict(G.degree())
plt.hist(degrees.values(), bins=20, color='skyblue', alpha=0.7)
plt.title('Degree Distribution')
plt.xlabel('Degree')
plt.ylabel('Frequency')
plt.show()

# 2. Community Detection (using Louvain method)
communities = community.best_partition(G)
pos = nx.spring_layout(G)
colors = [communities[node] for node in G.nodes()]
nx.draw(G, pos, node_color=colors, with_labels=True, font_size=8, cmap=plt.cm.RdYlBu, node_size=50)
plt.title('Community Detection')
plt.show()

# 3. Centrality Measures (using betweenness centrality)
centrality = nx.betweenness_centrality(G)
node_color = [centrality[node] for node in G.nodes()]
pos = nx.spring_layout(G)
nx.draw(G, pos, node_color=node_color, with_labels=True, font_size=8, cmap=plt.cm.Blues, node_size=50)
plt.title('Betweenness Centrality')
plt.show()

# 4. Shortest Paths
source_node = 'user123'
target_node = 'user456'
shortest_path = nx.shortest_path(G, source=source_node, target=target_node)
pos = nx.spring_layout(G)
nx.draw(G, pos, with_labels=True, font_size=8, node_size=50)
nx.draw_networkx_nodes(G, pos, nodelist=shortest_path, node_color='red', node_size=100)
nx.draw_networkx_edges(G, pos, edgelist=[(shortest_path[i], shortest_path[i+1]) for i in range(len(shortest_path)-1)], edge_color='red', width=2)
plt.title('Shortest Path')
plt.show()

# 5. Interactive Visualization (using pyvis)
from pyvis.network import Network
net = Network(notebook=True)
net.from_nx(G)
net.show('network.html')

import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt

# Load your Twitch Social Network dataset into a DataFrame
# Replace 'twitch_social_network.csv' with your actual file path
df = pd.read_csv('FR_edges.csv')

# Create a directed graph from the "from" and "to" columns
G = nx.from_pandas_edgelist(df, 'from', 'to', create_using=nx.DiGraph())

# Draw the force-directed graph
plt.figure(figsize=(12, 8))
pos = nx.spring_layout(G)
nx.draw(G, pos, with_labels=True, font_size=8, node_size=300, node_color='skyblue', font_color='black', edge_color='gray', alpha=0.7)
plt.title('Force-Directed Graph')
plt.show()

from nxviz import CircosPlot

# Create a Circos plot
c = CircosPlot(G, figsize=(12, 12))
c.draw()
plt.title('Circos Plot')
plt.show()

from nxviz import ArcPlot

# Create an Arc plot
a = ArcPlot(G, figsize=(12, 8))
a.draw()
plt.title('Arc Plot')
plt.show()

from nxviz import MatrixPlot

# Create a Matrix plot
m = MatrixPlot(G, figsize=(12, 8))
m.draw()
plt.title('Matrix Plot')
plt.show()

!pip install plotly

import pandas as pd
import plotly.express as px

# Load your dataset with hierarchical information
# Replace 'your_dataset.csv' with your actual file path and adjust the columns
df = pd.read_csv('FR_edges.csv')

# Assuming you have columns like 'parent', 'node', and 'value' in your dataset
fig = px.treemap(df, path=['from', 'to'], values='value')

# Show the figure
fig.show()

import pandas as pd
import plotly.express as px

# Load your dataset with "from" and "to" columns
# Replace 'your_dataset.csv' with your actual file path and adjust the columns
df = pd.read_csv('FR_edges.csv')

# Assuming you have columns like 'from' and 'to' in your dataset
fig = px.treemap(df, path=['from', 'to'])

# Show the figure
fig.show()

import pandas as pd
import networkx as nx
import plotly.express as px

# Load your Twitch Social Network dataset into a DataFrame
# Replace 'twitch_social_network.csv' with your actual file path
df = pd.read_csv('FR_edges.csv')

# Create a directed graph from the "from" and "to" columns
G = nx.from_pandas_edgelist(df, 'from', 'to', create_using=nx.DiGraph())

# Find the top 50 nodes with the highest degree (most connections)
top_50_nodes = [node for node, _ in sorted(G.degree, key=lambda x: x[1], reverse=True)[:50]]

# Create a subgraph with only the top 50 nodes and their edges
subgraph = G.subgraph(top_50_nodes)

# Convert the subgraph to a DataFrame to use in the treemap
subgraph_df = pd.DataFrame(subgraph.edges, columns=['from', 'to'])

# Create a treemap
fig = px.treemap(subgraph_df, path=['from', 'to'])

# Show the figure
fig.show()

pip install nxviz

import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt

# Load your Twitch Social Network dataset into a DataFrame
# Replace 'twitch_social_network.csv' with your actual file path
df = pd.read_csv('FR_edges.csv')

# Create a directed graph from the "from" and "to" columns
G = nx.from_pandas_edgelist(df, 'from', 'to', create_using=nx.DiGraph())

# Find the top 50 nodes with the highest degree (most connections)
top_50_nodes = [node for node, _ in sorted(G.degree, key=lambda x: x[1], reverse=True)[:50]]

# Create a subgraph with only the top 50 nodes and their edges
subgraph = G.subgraph(top_50_nodes)

# Create a space-filling graph using the Graph layout
plt.figure(figsize=(12, 12))
pos = nx.spring_layout(subgraph, seed=42)
nx.draw(subgraph, pos, with_labels=True, font_size=8, node_size=300, node_color='skyblue', font_color='black', edge_color='gray', alpha=0.7)
plt.title('Space-Filling Graph for Top 50 Nodes')
plt.show()

# Load your Twitch Social Network dataset into a DataFrame
# Replace 'twitch_social_network.csv' with your actual file path
df = pd.read_csv('FR_edges.csv')
# Take only the first 30 rows of the dataset
df_subset = df.head(100)

# Create a directed graph from the subset of the dataset
G = nx.from_pandas_edgelist(df_subset, 'from', 'to', create_using=nx.DiGraph())

# Plot the network for the first 30 rows
plt.figure(figsize=(12, 8))
pos = nx.spring_layout(G)
nx.draw(G, pos, with_labels=True, font_size=8, node_size=20, node_color='skyblue', font_color='black', edge_color='gray', alpha=0.7)
plt.title('Twitch Social Network Visualization')
plt.show()

import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt

# Load your Twitch Social Network dataset into a DataFrame
# Replace 'twitch_social_network.csv' with your actual file path
df = pd.read_csv('FR_edges.csv')

# Take only the first 500 rows of the dataset
df_subset = df.head(500)

# Create a directed graph from the subset of the dataset
G = nx.from_pandas_edgelist(df_subset, 'from', 'to', create_using=nx.DiGraph())

# Plot the network with multiple layouts for the first 500 rows
plt.figure(figsize=(18, 6))

# Spring layout
plt.subplot(1, 3, 1)
pos_spring = nx.spring_layout(G)
nx.draw(G, pos_spring, with_labels=True, font_size=8, node_size=20, node_color='skyblue', font_color='black', edge_color='gray', alpha=0.7)
plt.title('Spring Layout')

# Shell layout
plt.subplot(1, 3, 2)
pos_shell = nx.shell_layout(G)
nx.draw(G, pos_shell, with_labels=True, font_size=8, node_size=20, node_color='skyblue', font_color='black', edge_color='gray', alpha=0.7)
plt.title('Shell Layout')

# Circular layout
plt.subplot(1, 3, 3)
pos_circular = nx.circular_layout(G)
nx.draw(G, pos_circular, with_labels=True, font_size=8, node_size=20, node_color='skyblue', font_color='black', edge_color='gray', alpha=0.7)
plt.title('Circular Layout')

plt.tight_layout()
plt.show()

import pandas as pd
import networkx as nx
import plotly.express as px

# Load your Twitch Social Network dataset into a DataFrame
# Replace 'twitch_social_network.csv' with your actual file path
df = pd.read_csv('FR_edges.csv')

# Create a directed graph from the "from" and "to" columns
G = nx.from_pandas_edgelist(df, 'from', 'to', create_using=nx.DiGraph())

# Find the top 50 nodes with the highest degree (most connections)
top_50_nodes = [node for node, _ in sorted(G.degree, key=lambda x: x[1], reverse=True)[:50]]

# Create a subgraph with only the top 50 nodes and their edges
subgraph = G.subgraph(top_50_nodes)

# Convert the subgraph to a DataFrame to use in the treemap
subgraph_df = pd.DataFrame(subgraph.edges, columns=['from', 'to'])

# Create a treemap
fig = px.treemap(subgraph_df, path=['from', 'to'])

# Show the figure
fig.show()

pip install nltk wordcloud

import nltk
from wordcloud import WordCloud
import matplotlib.pyplot as plt

import pandas as pd
import nltk
from wordcloud import WordCloud
import matplotlib.pyplot as plt
nltk.download('punkt')
nltk.download('stopwords')

import pandas as pd
import re

import pandas as pd
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from nltk.tokenize import word_tokenize
from collections import Counter

import pandas as pd

# Assuming you have a DataFrame 'df' and you want to remove rows with float values in a specific column

# Function to check if a value is a float
def is_float(val):
    return isinstance(val, float)

# Column to check for float values
column_name = 'Positive_Review'  # Replace with the name of the column you want to check

# Remove rows with float values in the specified column
df = df[~df[column_name].apply(is_float)]

# Reset the DataFrame index if needed
df.reset_index(drop=True, inplace=True)

# Define a function to remove non-letter characters, periods, and parentheses
def remove_non_letters(text):
    # Replace periods and parentheses with spaces
    text = re.sub(r'<>*[.()]+', ' ', text)
    # Remove any character that is not an uppercase or lowercase letter (A-Z or a-z) or space
    return re.sub(r'[^a-zA-Z ]', '', text)

# Apply the function to the "review" column
df['Positive_Review'] = df['Positive_Review'].apply(remove_non_letters)

df['tokens'] = df['Positive_Review'].apply(lambda text: word_tokenize(str(text)))

pip install wordcloud

# Flatten the list of tokens and create a BoW representation
bow = Counter(word for sublist in df['tokens'] for word in sublist)

import pandas as pd
from nltk import pos_tag
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming you have the BoW 'bow' containing words and their frequencies

# Create a DataFrame from the BoW
df_word_pos = pd.DataFrame(list(bow.items()), columns=['Word', 'Count'])

# Perform POS tagging on the words
df_word_pos['POS'] = df_word_pos['Word'].apply(lambda word: pos_tag([word])[0][1])

# Get the top N most used words
top_n = 15  # Change this value to get the top N words
top_words = df_word_pos.nlargest(top_n, 'Count')

# Create the heatmap
plt.figure(figsize=(12, 6))
sns.heatmap(top_words.pivot("Word", "POS", "Count"), cmap="YlGnBu", annot=True, fmt="d", cbar_kws={'label': 'Frequency'})
plt.title("POS Heatmap with Top {} Used Words and Word Frequencies".format(top_n))
plt.show()

wordcloud = WordCloud(width=800, height=400, max_words=100, background_color='white').generate_from_frequencies(bow)

# Display the word cloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

# Download the NLTK stop words
nltk.download('stopwords')

from nltk.corpus import stopwords

stop_words = set(stopwords.words('english'))

def remove_stop_words(tokens):
    return [word for word in tokens if word.lower() not in stop_words]

df['tokens'] = df['Positive_Review'].apply(lambda text: word_tokenize(str(text)))
df['tokens'] = df['tokens'].apply(remove_stop_words)

bow = Counter(word for sublist in df['tokens'] for word in sublist)

import pandas as pd
import matplotlib.pyplot as plt
from collections import Counter

# Assuming you have a DataFrame "df" with "Reviewer_Nationality" and "Positive_Review" columns
# Replace this with your actual data

# Assuming you have a BoW represented as a dictionary "bow"
# Replace this with your actual BoW representation

# Step 1: Filter the dataset for the top ten most frequent countries
top_countries = df['Reviewer_Nationality'].value_counts().head(10).index
filtered_df = df[df['Reviewer_Nationality'].isin(top_countries)]

# Create subplots for each country
fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(12, 10))
fig.suptitle('Most Common Words in Negative Reviews by Country', fontsize=16)

for i, country in enumerate(top_countries):
    row, col = i // 2, i % 2
    ax = axes[row, col]

    country_df = filtered_df[filtered_df['Reviewer_Nationality'] == country]
    country_positive_reviews = country_df['Negative_Review']

    # Create a dictionary to store word frequencies for the country
    country_word_frequencies = Counter()

    for review in country_positive_reviews:
        words = review.split()
        for word in words:
            if word in bow:
                country_word_frequencies[word] += 1

    # Find the top 10 most common words
    top_words = country_word_frequencies.most_common(10)
    words, frequencies = zip(*top_words)

    # Create a bar chart for the current country
    ax.barh(words, frequencies)
    ax.set_title(country)
    ax.set_xlabel('Frequency')

# Adjust spacing and show the plot
plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from collections import Counter

# Assuming you have a DataFrame "df" with "Reviewer_Nationality" and "Positive_Review" columns
# Replace this with your actual data

# Assuming you have a BoW represented as a dictionary "bow"
# Replace this with your actual BoW representation

# Step 1: Filter the dataset for the top ten most frequent countries
top_countries = df['Reviewer_Nationality'].value_counts().head(10).index
filtered_df = df[df['Reviewer_Nationality'].isin(top_countries)]

# Create subplots for each country
fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(12, 10))
fig.suptitle('Most Common Words in Negative Reviews by Hotel', fontsize=16)

for i, country in enumerate(top_countries):
    row, col = i // 2, i % 2
    ax = axes[row, col]

    country_df = filtered_df[filtered_df['Reviewer_Nationality'] == country]
    country_positive_reviews = country_df['Negative_Review']

    # Create a dictionary to store word frequencies for the country
    country_word_frequencies = Counter()

    for review in country_positive_reviews:
        words = review.split()
        for word in words:
            if word in bow:
                country_word_frequencies[word] += 1

    # Find the top 10 most common words
    top_words = country_word_frequencies.most_common(10)
    words, frequencies = zip(*top_words)

    # Create a bar chart for the current country
    ax.barh(words, frequencies)
    ax.set_title(country)
    ax.set_xlabel('Frequency')

# Adjust spacing and show the plot
plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()

import matplotlib.pyplot as plt

# Sample BoW with words and their frequencies
# Replace this with your actual BoW

# Get the 10 most used words and their frequencies
top_words = dict(sorted(bow.items(), key=lambda item: item[1], reverse=True)[:10])

# Calculate the total frequency of the top 10 words
total_frequency = sum(top_words.values())

# Calculate the percentages for the top 10 words
percentages = [freq / total_frequency * 100 for freq in top_words.values()]

# Create a pie chart for the top 10 words
plt.figure(figsize=(6, 6))
labels = top_words.keys()
sizes = percentages
plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140)
plt.title('Frequency of the 10 Most Used Words (as Percentages)')

# Display the pie chart
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Assuming you have a BoW 'bow' with words as keys and their corresponding frequencies as values

# Create a DataFrame from the BoW
df_bow = pd.DataFrame(list(bow.items()), columns=['Word', 'Frequency'])

# Plot a histogram of word frequencies
plt.figure(figsize=(12, 6))
plt.hist(df_bow['Frequency'], bins=50, range=(0, 50), color='blue', edgecolor='black')
plt.xlabel('Word Frequency')
plt.ylabel('Number of Words')
plt.title('Histogram of Word Frequencies (After Stop Words Removal)')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

import pandas as pd
from nltk import pos_tag
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming you have the BoW 'bow' containing words and their frequencies

# Create a DataFrame from the BoW
df_word_pos = pd.DataFrame(list(bow.items()), columns=['Word', 'Count'])

# Perform POS tagging on the words
df_word_pos['POS'] = df_word_pos['Word'].apply(lambda word: pos_tag([word])[0][1])

# Get the top N most used words
top_n = 15  # Change this value to get the top N words
top_words = df_word_pos.nlargest(top_n, 'Count')

# Create the heatmap
plt.figure(figsize=(12, 6))
sns.heatmap(top_words.pivot("Word", "POS", "Count"), cmap="YlGnBu", annot=True, fmt="d", cbar_kws={'label': 'Frequency'})
plt.title("POS Heatmap with Top {} Used Words and Word Frequencies".format(top_n))
plt.show()

# Create a word cloud
wordcloud = WordCloud(width=800, height=400, max_words=100, background_color='white').generate_from_frequencies(bow)

# Display the word cloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

from collections import Counter
import matplotlib.pyplot as plt

# Access the most common words and their frequencies
common_words = [word for word, freq in bow.most_common(20)]  # Display the top 20 words
word_frequencies = [freq for word, freq in bow.most_common(20)]

# Create a frequency chart (bar chart)
plt.figure(figsize=(10, 5))
plt.bar(common_words, word_frequencies)
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.title('Word Frequencies in Text Data')
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability
plt.tight_layout()
plt.show()

import nltk
nltk.download('averaged_perceptron_tagger')

pip install nltk

import nltk
nltk.download('averaged_perceptron_tagger')

import pandas as pd
from nltk import pos_tag
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns
import nltk

# Load the Hotel_Reviews CSV file
df = pd.read_csv('Hotel_Reviews.csv')

# Define a set of English stop words
stop_words = set(stopwords.words('english'))

# Function to tokenize and filter stop words
def tokenize_and_filter(text):
    tokens = word_tokenize(text)
    return [word for word in tokens if word.lower() not in stop_words]

# Combine all positive reviews into a single text
positive_reviews = ' '.join(df['Positive_Review'])

# Tokenize and perform POS tagging while filtering stop words
filtered_tokens = tokenize_and_filter(positive_reviews)
pos_tags = pos_tag(filtered_tokens)

# Create a list of (word, POS) tuples
word_pos_tuples = [(word, tag) for word, tag in pos_tags]

# Count the occurrences of (word, POS) tuples
word_pos_counts = Counter(word_pos_tuples)

# Create a DataFrame
df_word_pos = pd.DataFrame(word_pos_counts.most_common(), columns=['Word-POS', 'Count'])

# Split the 'Word-POS' column into 'Word' and 'POS' columns
df_word_pos[['Word', 'POS']] = df_word_pos['Word-POS'].str.split('-', expand=True)

# Get the top 15 used words
top_words = df_word_pos['Word'].value_counts().head(15).index

# Filter the DataFrame to include only the top 15 words
df_word_pos = df_word_pos[df_word_pos['Word'].isin(top_words)]

# Create the heatmap
plt.figure(figsize=(12, 6))
sns.heatmap(df_word_pos.pivot("Word", "POS", "Count"), cmap="YlGnBu", annot=True, fmt="d", cbar_kws={'label': 'Frequency'})
plt.title("POS Heatmap with Top 15 Used Words and Word Frequencies (Positive Reviews)")
plt.show()

from nltk import pos_tag
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

import nltk

# Download the averaged_perceptron_tagger resource
nltk.download('averaged_perceptron_tagger')

# Assuming you have a BoW representation named 'bow'
tokens = [word for sublist in df['tokens'] for word in sublist]

# Perform POS tagging on the tokens
pos_tags = pos_tag(tokens)

pos_tag_counts = Counter(tag for word, tag in pos_tags)

# Convert the Counter to a DataFrame
pos_df = pd.DataFrame.from_dict(pos_tag_counts, orient='index').reset_index()
pos_df.columns = ['POS Tag', 'Count']

sns.set(style="whitegrid")
plt.figure(figsize=(12, 6))
heatmap = sns.heatmap(pos_df.pivot("POS Tag", "Count", "Count"), annot=True, fmt="d", cmap="YlGnBu")
plt.title("POS Tag Heatmap")
plt.show()

# Assuming you have a BoW representation named 'bow'
tokens = [word for sublist in df['tokens'] for word in sublist]

# Perform POS tagging on the tokens
pos_tags = pos_tag(tokens)

from nltk import pos_tag
from nltk.tokenize import word_tokenize
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import nltk

# Tokenize the text data
tokens = [word for sublist in df['tokens'] for word in sublist]

# Perform POS tagging on the tokens
pos_tags = pos_tag(tokens)

# Create a list of (word, POS) tuples
word_pos_tuples = [(word, tag) for word, tag in pos_tags]

# Count the occurrences of (word, POS) tuples
word_pos_counts = Counter(word_pos_tuples)

# Create a DataFrame
df_word_pos = pd.DataFrame(word_pos_counts.most_common(), columns=['Word-POS', 'Count'])

# Split the 'Word-POS' column into 'Word' and 'POS' columns
df_word_pos[['Word', 'POS']] = df_word_pos['Word-POS'].str.split('-', expand=True)

# Pivot the DataFrame for the heatmap
pivot_table = df_word_pos.pivot_table(index='Word', columns='POS', values='Count', fill_value=0)

# Create the heatmap
plt.figure(figsize=(12, 6))
sns.heatmap(pivot_table, cmap="YlGnBu", annot=True, fmt="d")
plt.title("Word-POS Tag Heatmap with Word Frequencies")
plt.show()