# -*- coding: utf-8 -*-
"""FinalP2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/162eBbosQYwlr_HuhFNerF8RxST1-LrTY
"""

!pip install researchpy

import statistics as st # Built in Python library for descriptive statistics
import pandas as pd  # Data manipulation and analysis library
import researchpy as rp  # Open source library focused on univariate and bivariate analysis
import numpy as np  # General purpose array processing package
import seaborn as sns  # Data visualization library based on matplotlib
import matplotlib.pyplot as plt  # Data visualization library
import scipy.stats as spy  # Statistical library from Scipy
import statistics as st # Built in Python library for descriptive statistics
import pandas as pd  # Data manipulation and analysis library
import researchpy as rp  # Open source library focused on univariate and bivariate analysis
import numpy as np  # General purpose array processing package
import seaborn as sns  # Data visualization library based on matplotlib
import matplotlib.pyplot as plt  # Data visualization library
import scipy.stats as spy  # Statistical library from Scipy
from scipy.stats import norm, kurtosis, laplace, semicircular
from scipy import stats
from scipy.stats import gmean, hmean, trim_mean

# Loading dataset1
# https://archive.ics.uci.edu/ml/datasets/Mammographic+Mass
# Missing Values? Yes
df = pd.read_csv('Airline_review.csv')
df.shape

pip install nltk wordcloud

import nltk
from wordcloud import WordCloud
import matplotlib.pyplot as plt

import pandas as pd
import nltk
from wordcloud import WordCloud
import matplotlib.pyplot as plt
nltk.download('punkt')
nltk.download('stopwords')

import pandas as pd
import re

import pandas as pd
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from nltk.tokenize import word_tokenize
from collections import Counter

import pandas as pd

# Assuming you have a DataFrame 'df' and you want to remove rows with float values in a specific column

# Function to check if a value is a float
def is_float(val):
    return isinstance(val, float)

# Column to check for float values
column_name = 'Review'  # Replace with the name of the column you want to check

# Remove rows with float values in the specified column
df = df[~df[column_name].apply(is_float)]

# Reset the DataFrame index if needed
df.reset_index(drop=True, inplace=True)

# Define a function to remove non-letter characters, periods, and parentheses
def remove_non_letters(text):
    # Replace periods and parentheses with spaces
    text = re.sub(r'<>*[.()]+', ' ', text)
    # Remove any character that is not an uppercase or lowercase letter (A-Z or a-z) or space
    return re.sub(r'[^a-zA-Z ]', '', text)

# Apply the function to the "review" column
df['Review'] = df['Review'].apply(remove_non_letters)

df['tokens'] = df['Review'].apply(lambda text: word_tokenize(str(text)))

pip install wordcloud

# Flatten the list of tokens and create a BoW representation
bow = Counter(word for sublist in df['tokens'] for word in sublist)

import nltk
nltk.download('averaged_perceptron_tagger')

import pandas as pd
from nltk import pos_tag
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming you have the BoW 'bow' containing words and their frequencies

# Create a DataFrame from the BoW
df_word_pos = pd.DataFrame(list(bow.items()), columns=['Word', 'Count'])

# Perform POS tagging on the words
df_word_pos['POS'] = df_word_pos['Word'].apply(lambda word: pos_tag([word])[0][1])

# Get the top N most used words
top_n = 15  # Change this value to get the top N words
top_words = df_word_pos.nlargest(top_n, 'Count')

# Create the heatmap
plt.figure(figsize=(12, 6))
sns.heatmap(top_words.pivot("Word", "POS", "Count"), cmap="YlGnBu", annot=True, fmt="d", cbar_kws={'label': 'Frequency'})
plt.title("POS Heatmap with Top {} Used Words and Word Frequencies".format(top_n))
plt.show()

wordcloud = WordCloud(width=800, height=400, max_words=100, background_color='white').generate_from_frequencies(bow)

# Display the word cloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

# Download the NLTK stop words
nltk.download('stopwords')

from nltk.corpus import stopwords

stop_words = set(stopwords.words('english'))

def remove_stop_words(tokens):
    return [word for word in tokens if word.lower() not in stop_words]

df['tokens'] = df['Review'].apply(lambda text: word_tokenize(str(text)))
df['tokens'] = df['tokens'].apply(remove_stop_words)

bow = Counter(word for sublist in df['tokens'] for word in sublist)

import pandas as pd
from nltk import pos_tag
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming you have the BoW 'bow' containing words and their frequencies

# Create a DataFrame from the BoW
df_word_pos = pd.DataFrame(list(bow.items()), columns=['Word', 'Count'])

# Perform POS tagging on the words
df_word_pos['POS'] = df_word_pos['Word'].apply(lambda word: pos_tag([word])[0][1])

# Get the top N most used words
top_n = 15  # Change this value to get the top N words
top_words = df_word_pos.nlargest(top_n, 'Count')

# Create the heatmap
plt.figure(figsize=(12, 6))
sns.heatmap(top_words.pivot("Word", "POS", "Count"), cmap="YlGnBu", annot=True, fmt="d", cbar_kws={'label': 'Frequency'})
plt.title("POS Heatmap with Top {} Used Words and Word Frequencies".format(top_n))
plt.show()

wordcloud = WordCloud(width=800, height=400, max_words=100, background_color='white').generate_from_frequencies(bow)

# Display the word cloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

import pandas as pd
import nltk
from nltk.corpus import stopwords
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('Airline_review.csv')

# Identify the top 10 most reviewed airlines
top_10_airlines = df['Airline Name'].value_counts().nlargest(10).index

# Filter the dataset for the top 10 airlines
df_top_10 = df[df['Airline Name'].isin(top_10_airlines)]

# Tokenize and remove stopwords and non-letters
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    words = nltk.word_tokenize(text)
    words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]
    return words

df_top_10['ProcessedReview'] = df_top_10['Review'].apply(preprocess_text)

# Create a DataFrame with airline names and processed reviews
processed_df = pd.DataFrame({'Airline Name': df_top_10['Airline Name'], 'ProcessedReview': df_top_10['ProcessedReview']})

# Group by airline and calculate word frequencies
word_frequencies = processed_df.groupby('Airline Name')['ProcessedReview'].apply(lambda x: pd.Series(x).sum()).apply(pd.Series).stack().value_counts()

# Plot the bar chart
plt.figure(figsize=(10, 6))
word_frequencies.plot(kind='bar')
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.title('Most Frequently Used Words by Top 10 Airlines (After Preprocessing)')
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from collections import Counter

# Assuming you have a DataFrame "df" with "Reviewer_Nationality" and "Positive_Review" columns
# Replace this with your actual data

# Assuming you have a BoW represented as a dictionary "bow"
# Replace this with your actual BoW representation

# Step 1: Filter the dataset for the top ten most frequent countries
top_countries = df['Reviewer_Nationality'].value_counts().head(10).index
filtered_df = df[df['Reviewer_Nationality'].isin(top_countries)]

# Create subplots for each country
fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(12, 10))
fig.suptitle('Most Common Words in Negative Reviews by Country', fontsize=16)

for i, country in enumerate(top_countries):
    row, col = i // 2, i % 2
    ax = axes[row, col]

    country_df = filtered_df[filtered_df['Reviewer_Nationality'] == country]
    country_positive_reviews = country_df['Negative_Review']

    # Create a dictionary to store word frequencies for the country
    country_word_frequencies = Counter()

    for review in country_positive_reviews:
        words = review.split()
        for word in words:
            if word in bow:
                country_word_frequencies[word] += 1

    # Find the top 10 most common words
    top_words = country_word_frequencies.most_common(10)
    words, frequencies = zip(*top_words)

    # Create a bar chart for the current country
    ax.barh(words, frequencies)
    ax.set_title(country)
    ax.set_xlabel('Frequency')

# Adjust spacing and show the plot
plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from collections import Counter

# Assuming you have a DataFrame "df" with "Reviewer_Nationality" and "Positive_Review" columns
# Replace this with your actual data

# Assuming you have a BoW represented as a dictionary "bow"
# Replace this with your actual BoW representation

# Step 1: Filter the dataset for the top ten most frequent countries
top_countries = df['Reviewer_Nationality'].value_counts().head(10).index
filtered_df = df[df['Reviewer_Nationality'].isin(top_countries)]

# Create subplots for each country
fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(12, 10))
fig.suptitle('Most Common Words in Negative Reviews by Hotel', fontsize=16)

for i, country in enumerate(top_countries):
    row, col = i // 2, i % 2
    ax = axes[row, col]

    country_df = filtered_df[filtered_df['Reviewer_Nationality'] == country]
    country_positive_reviews = country_df['Negative_Review']

    # Create a dictionary to store word frequencies for the country
    country_word_frequencies = Counter()

    for review in country_positive_reviews:
        words = review.split()
        for word in words:
            if word in bow:
                country_word_frequencies[word] += 1

    # Find the top 10 most common words
    top_words = country_word_frequencies.most_common(10)
    words, frequencies = zip(*top_words)

    # Create a bar chart for the current country
    ax.barh(words, frequencies)
    ax.set_title(country)
    ax.set_xlabel('Frequency')

# Adjust spacing and show the plot
plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()

import matplotlib.pyplot as plt

# Sample BoW with words and their frequencies
# Replace this with your actual BoW

# Get the 10 most used words and their frequencies
top_words = dict(sorted(bow.items(), key=lambda item: item[1], reverse=True)[:10])

# Calculate the total frequency of the top 10 words
total_frequency = sum(top_words.values())

# Calculate the percentages for the top 10 words
percentages = [freq / total_frequency * 100 for freq in top_words.values()]

# Create a pie chart for the top 10 words
plt.figure(figsize=(6, 6))
labels = top_words.keys()
sizes = percentages
plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140)
plt.title('Frequency of the 10 Most Used Words (as Percentages)')

# Display the pie chart
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Assuming you have a BoW 'bow' with words as keys and their corresponding frequencies as values

# Create a DataFrame from the BoW
df_bow = pd.DataFrame(list(bow.items()), columns=['Word', 'Frequency'])

# Plot a histogram of word frequencies
plt.figure(figsize=(12, 6))
plt.hist(df_bow['Frequency'], bins=50, range=(0, 50), color='blue', edgecolor='black')
plt.xlabel('Word Frequency')
plt.ylabel('Number of Words')
plt.title('Histogram of Word Frequencies (After Stop Words Removal)')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

import pandas as pd
from nltk import pos_tag
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming you have the BoW 'bow' containing words and their frequencies

# Create a DataFrame from the BoW
df_word_pos = pd.DataFrame(list(bow.items()), columns=['Word', 'Count'])

# Perform POS tagging on the words
df_word_pos['POS'] = df_word_pos['Word'].apply(lambda word: pos_tag([word])[0][1])

# Get the top N most used words
top_n = 15  # Change this value to get the top N words
top_words = df_word_pos.nlargest(top_n, 'Count')

# Create the heatmap
plt.figure(figsize=(12, 6))
sns.heatmap(top_words.pivot("Word", "POS", "Count"), cmap="YlGnBu", annot=True, fmt="d", cbar_kws={'label': 'Frequency'})
plt.title("POS Heatmap with Top {} Used Words and Word Frequencies".format(top_n))
plt.show()

# Create a word cloud
wordcloud = WordCloud(width=800, height=400, max_words=100, background_color='white').generate_from_frequencies(bow)

# Display the word cloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

from collections import Counter
import matplotlib.pyplot as plt

# Access the most common words and their frequencies
common_words = [word for word, freq in bow.most_common(20)]  # Display the top 20 words
word_frequencies = [freq for word, freq in bow.most_common(20)]

# Create a frequency chart (bar chart)
plt.figure(figsize=(10, 5))
plt.bar(common_words, word_frequencies)
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.title('Word Frequencies in Text Data')
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability
plt.tight_layout()
plt.show()

import nltk
nltk.download('averaged_perceptron_tagger')

pip install nltk

import nltk
nltk.download('averaged_perceptron_tagger')

import pandas as pd
from nltk import pos_tag
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns
import nltk

# Load the Hotel_Reviews CSV file
df = pd.read_csv('Hotel_Reviews.csv')

# Define a set of English stop words
stop_words = set(stopwords.words('english'))

# Function to tokenize and filter stop words
def tokenize_and_filter(text):
    tokens = word_tokenize(text)
    return [word for word in tokens if word.lower() not in stop_words]

# Combine all positive reviews into a single text
positive_reviews = ' '.join(df['Positive_Review'])

# Tokenize and perform POS tagging while filtering stop words
filtered_tokens = tokenize_and_filter(positive_reviews)
pos_tags = pos_tag(filtered_tokens)

# Create a list of (word, POS) tuples
word_pos_tuples = [(word, tag) for word, tag in pos_tags]

# Count the occurrences of (word, POS) tuples
word_pos_counts = Counter(word_pos_tuples)

# Create a DataFrame
df_word_pos = pd.DataFrame(word_pos_counts.most_common(), columns=['Word-POS', 'Count'])

# Split the 'Word-POS' column into 'Word' and 'POS' columns
df_word_pos[['Word', 'POS']] = df_word_pos['Word-POS'].str.split('-', expand=True)

# Get the top 15 used words
top_words = df_word_pos['Word'].value_counts().head(15).index

# Filter the DataFrame to include only the top 15 words
df_word_pos = df_word_pos[df_word_pos['Word'].isin(top_words)]

# Create the heatmap
plt.figure(figsize=(12, 6))
sns.heatmap(df_word_pos.pivot("Word", "POS", "Count"), cmap="YlGnBu", annot=True, fmt="d", cbar_kws={'label': 'Frequency'})
plt.title("POS Heatmap with Top 15 Used Words and Word Frequencies (Positive Reviews)")
plt.show()

from nltk import pos_tag
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

import nltk

# Download the averaged_perceptron_tagger resource
nltk.download('averaged_perceptron_tagger')

# Assuming you have a BoW representation named 'bow'
tokens = [word for sublist in df['tokens'] for word in sublist]

# Perform POS tagging on the tokens
pos_tags = pos_tag(tokens)

pos_tag_counts = Counter(tag for word, tag in pos_tags)

# Convert the Counter to a DataFrame
pos_df = pd.DataFrame.from_dict(pos_tag_counts, orient='index').reset_index()
pos_df.columns = ['POS Tag', 'Count']

sns.set(style="whitegrid")
plt.figure(figsize=(12, 6))
heatmap = sns.heatmap(pos_df.pivot("POS Tag", "Count", "Count"), annot=True, fmt="d", cmap="YlGnBu")
plt.title("POS Tag Heatmap")
plt.show()

# Assuming you have a BoW representation named 'bow'
tokens = [word for sublist in df['tokens'] for word in sublist]

# Perform POS tagging on the tokens
pos_tags = pos_tag(tokens)

from nltk import pos_tag
from nltk.tokenize import word_tokenize
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import nltk

# Tokenize the text data
tokens = [word for sublist in df['tokens'] for word in sublist]

# Perform POS tagging on the tokens
pos_tags = pos_tag(tokens)

# Create a list of (word, POS) tuples
word_pos_tuples = [(word, tag) for word, tag in pos_tags]

# Count the occurrences of (word, POS) tuples
word_pos_counts = Counter(word_pos_tuples)

# Create a DataFrame
df_word_pos = pd.DataFrame(word_pos_counts.most_common(), columns=['Word-POS', 'Count'])

# Split the 'Word-POS' column into 'Word' and 'POS' columns
df_word_pos[['Word', 'POS']] = df_word_pos['Word-POS'].str.split('-', expand=True)

# Pivot the DataFrame for the heatmap
pivot_table = df_word_pos.pivot_table(index='Word', columns='POS', values='Count', fill_value=0)

# Create the heatmap
plt.figure(figsize=(12, 6))
sns.heatmap(pivot_table, cmap="YlGnBu", annot=True, fmt="d")
plt.title("Word-POS Tag Heatmap with Word Frequencies")
plt.show()